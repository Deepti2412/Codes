{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation with LSTM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNaT3pHemvSawyQEt38mHzV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepti2412/Codes/blob/master/Text_Generation_with_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEqU8sqFKUvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('moby_dick_four_chapters.txt','r') as f:\n",
        "  r=f.read()\n",
        "print(r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyy9210pK9d0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b3949170-24ba-45b9-efda-4afcc3e9ec96"
      },
      "source": [
        "r[:50]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Call me Ishmael.  Some years ago--never mind how l'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTBjITOqLBsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4QtfvKjLG_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp=spacy.load('en',diable=['parser','tagger','ner'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9-4i6xBLXCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp.max_length=1198623"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvCjVy-wLcAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def separate_punct(doc):\n",
        "  return [token.text.lower() for token in nlp(doc) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcbnnN59L922",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens=separate_punct(r)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPeQIVrCMD9W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed0cabde-c8d8-4933-d7ed-d76a9375c055"
      },
      "source": [
        "len(tokens)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11338"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vsleak9Mm31",
        "colab_type": "text"
      },
      "source": [
        "Create Sequences of Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WapF-BOMfX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_len= 25 + 1\n",
        "text_sequences=[]\n",
        "for i in range(train_len,len(tokens)):\n",
        "  sequences=tokens[i-train_len:i]\n",
        "  text_sequences.append(sequences)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXz4QhHTNMUD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bb7b60fa-9126-438d-cffd-e3910e8f6d0c"
      },
      "source": [
        "\" \".join(text_sequences[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'call me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af3HSRW4Nfh_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d0985921-4799-4836-bcce-5cc7ae7bbf27"
      },
      "source": [
        "\" \".join(text_sequences[1])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDo3ITLyNkAO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb09fcb2-db18-4971-d6f4-832715904b15"
      },
      "source": [
        "len(text_sequences)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11312"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IidjZy-fNrit",
        "colab_type": "text"
      },
      "source": [
        "KERAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cfayRtlNpOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras_preprocessing.text import Tokenizer"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKpSjSTgN10L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer=Tokenizer()\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UsEvTdlOA-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.fit_on_texts(text_sequences)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCQRUrNeOFF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences=tokenizer.texts_to_sequences(text_sequences)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3NKF4vgOKnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNdkEcpJObnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.index_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmRV-EQIOuAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in sequences[0]:\n",
        "  print(f'{i} {tokenizer.index_word[i]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlJWTShGPD1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary_size=len(tokenizer.word_counts)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCyTEaWhUXAv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "13dcc693-8981-4e5d-9aa4-6556d56c5dbf"
      },
      "source": [
        "vocabulary_size"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2717"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svcPvzMJPTQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MbUYJsTPcYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences=np.array(sequences)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKsd4I-nPg1h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "c122755c-6d6a-4af7-f6d5-c42c8b52b0b1"
      },
      "source": [
        "sequences"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 956,   14,  263, ..., 2712,   14,   24],\n",
              "       [  14,  263,   51, ...,   14,   24,  957],\n",
              "       [ 263,   51,  261, ...,   24,  957,    5],\n",
              "       ...,\n",
              "       [ 952,   12,  166, ...,  262,   53,    2],\n",
              "       [  12,  166, 2711, ...,   53,    2, 2717],\n",
              "       [ 166, 2711,    3, ...,    2, 2717,   26]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS3N4RRmPkbr",
        "colab_type": "text"
      },
      "source": [
        "Creating an LSTM based model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo7PUx_zPhxE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "44559ca8-491f-4b9c-cb59-cb31734e1a68"
      },
      "source": [
        "from keras.layers import Dense,LSTM,Embedding\n",
        "from keras.models import Sequential"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq2kt3JQP4lZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(vocabulary_size,seq_len):\n",
        "  model=Sequential()\n",
        "  model.add(Embedding(vocabulary_size,25,input_length=seq_len))\n",
        "  model.add(LSTM(150,return_sequences=True))\n",
        "  model.add(LSTM(150))\n",
        "  model.add(Dense(150,activation='relu'))\n",
        "  model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "   \n",
        "  model.summary()\n",
        "    \n",
        "  return model"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFmWbrJ6TUkU",
        "colab_type": "text"
      },
      "source": [
        "Train / Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYz6ZH7iRB4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZx3T5RITbK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=sequences[:,:-1]\n",
        "y=sequences[:,-1]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NypttkAwThGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kshz2DxTlyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBW9zvDzTpfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B4CnhJJTydm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=to_categorical(y,num_classes=vocabulary_size+ 1)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbOpRYEGT6Fg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_len = X.shape[1]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0Ks63X3UjkL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "dbf17aaa-e33c-4642-9418-f1f88fab3b92"
      },
      "source": [
        "model = create_model(vocabulary_size+1, seq_len)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 25, 25)            67950     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 25, 150)           105600    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 150)               180600    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 150)               22650     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2718)              410418    \n",
            "=================================================================\n",
            "Total params: 787,218\n",
            "Trainable params: 787,218\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEIL4yz_Unor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pickle import dump,load"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X64YlkPRUrfN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "78722d88-9e0a-40da-a3d5-ab01a321aa7a"
      },
      "source": [
        "# fit model\n",
        "model.fit(X, y, batch_size=128, epochs=300,verbose=1)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "11312/11312 [==============================] - 25s 2ms/step - loss: 6.8471 - accuracy: 0.0463\n",
            "Epoch 2/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 6.3854 - accuracy: 0.0510\n",
            "Epoch 3/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 6.3274 - accuracy: 0.0522\n",
            "Epoch 4/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 6.1828 - accuracy: 0.0529\n",
            "Epoch 5/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 6.1014 - accuracy: 0.0542\n",
            "Epoch 6/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 5.9784 - accuracy: 0.0612\n",
            "Epoch 7/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 5.8620 - accuracy: 0.0670\n",
            "Epoch 8/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 5.7837 - accuracy: 0.0680\n",
            "Epoch 9/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 5.7071 - accuracy: 0.0693\n",
            "Epoch 10/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 5.6238 - accuracy: 0.0733\n",
            "Epoch 11/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 5.5557 - accuracy: 0.0775\n",
            "Epoch 12/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 5.4930 - accuracy: 0.0793\n",
            "Epoch 13/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 5.4430 - accuracy: 0.0813\n",
            "Epoch 14/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 5.3855 - accuracy: 0.0834\n",
            "Epoch 15/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 5.3358 - accuracy: 0.0835\n",
            "Epoch 16/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 5.2943 - accuracy: 0.0870\n",
            "Epoch 17/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 5.2458 - accuracy: 0.0864\n",
            "Epoch 18/300\n",
            "11312/11312 [==============================] - 30s 3ms/step - loss: 5.1973 - accuracy: 0.0893\n",
            "Epoch 19/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 5.1559 - accuracy: 0.0903\n",
            "Epoch 20/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 5.1170 - accuracy: 0.0893\n",
            "Epoch 21/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 5.0758 - accuracy: 0.0918\n",
            "Epoch 22/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 5.0323 - accuracy: 0.0918\n",
            "Epoch 23/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.9918 - accuracy: 0.0938\n",
            "Epoch 24/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.9491 - accuracy: 0.0944\n",
            "Epoch 25/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.9049 - accuracy: 0.0930\n",
            "Epoch 26/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.8623 - accuracy: 0.0954\n",
            "Epoch 27/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.8188 - accuracy: 0.0973\n",
            "Epoch 28/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.7762 - accuracy: 0.0955\n",
            "Epoch 29/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.7287 - accuracy: 0.0966\n",
            "Epoch 30/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.6867 - accuracy: 0.0960\n",
            "Epoch 31/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.6446 - accuracy: 0.1009\n",
            "Epoch 32/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.5975 - accuracy: 0.1016\n",
            "Epoch 33/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.5488 - accuracy: 0.1017\n",
            "Epoch 34/300\n",
            "11312/11312 [==============================] - 25s 2ms/step - loss: 4.5062 - accuracy: 0.1055\n",
            "Epoch 35/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.4660 - accuracy: 0.1073\n",
            "Epoch 36/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.4229 - accuracy: 0.1088\n",
            "Epoch 37/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.3740 - accuracy: 0.1087\n",
            "Epoch 38/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.3349 - accuracy: 0.1138\n",
            "Epoch 39/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.2893 - accuracy: 0.1154\n",
            "Epoch 40/300\n",
            "11312/11312 [==============================] - 23s 2ms/step - loss: 4.2424 - accuracy: 0.1139\n",
            "Epoch 41/300\n",
            "11312/11312 [==============================] - 23s 2ms/step - loss: 4.2006 - accuracy: 0.1200\n",
            "Epoch 42/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.1608 - accuracy: 0.1211\n",
            "Epoch 43/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.1134 - accuracy: 0.1284\n",
            "Epoch 44/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.0814 - accuracy: 0.1288\n",
            "Epoch 45/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.0449 - accuracy: 0.1276\n",
            "Epoch 46/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.0041 - accuracy: 0.1322\n",
            "Epoch 47/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.9747 - accuracy: 0.1353\n",
            "Epoch 48/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.9301 - accuracy: 0.1405\n",
            "Epoch 49/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.8961 - accuracy: 0.1398\n",
            "Epoch 50/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.8571 - accuracy: 0.1440\n",
            "Epoch 51/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.8191 - accuracy: 0.1514\n",
            "Epoch 52/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.7934 - accuracy: 0.1552\n",
            "Epoch 53/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.0498 - accuracy: 0.1407\n",
            "Epoch 54/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.8069 - accuracy: 0.1520\n",
            "Epoch 55/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.7498 - accuracy: 0.1615\n",
            "Epoch 56/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.7137 - accuracy: 0.1649\n",
            "Epoch 57/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.6747 - accuracy: 0.1693\n",
            "Epoch 58/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.6547 - accuracy: 0.1653\n",
            "Epoch 59/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.6133 - accuracy: 0.1766\n",
            "Epoch 60/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.5839 - accuracy: 0.1795\n",
            "Epoch 61/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.5692 - accuracy: 0.1795\n",
            "Epoch 62/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.5330 - accuracy: 0.1864\n",
            "Epoch 63/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.5034 - accuracy: 0.1911\n",
            "Epoch 64/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.4766 - accuracy: 0.1914\n",
            "Epoch 65/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.4527 - accuracy: 0.1971\n",
            "Epoch 66/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.4250 - accuracy: 0.2010\n",
            "Epoch 67/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.4000 - accuracy: 0.2019\n",
            "Epoch 68/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.3736 - accuracy: 0.2087\n",
            "Epoch 69/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.3434 - accuracy: 0.2085\n",
            "Epoch 70/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.3206 - accuracy: 0.2153\n",
            "Epoch 71/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.3023 - accuracy: 0.2157\n",
            "Epoch 72/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.2711 - accuracy: 0.2261\n",
            "Epoch 73/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.2564 - accuracy: 0.2279\n",
            "Epoch 74/300\n",
            "11312/11312 [==============================] - 23s 2ms/step - loss: 3.2264 - accuracy: 0.2304\n",
            "Epoch 75/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.2012 - accuracy: 0.2376\n",
            "Epoch 76/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.1754 - accuracy: 0.2392\n",
            "Epoch 77/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.1549 - accuracy: 0.2458\n",
            "Epoch 78/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.1414 - accuracy: 0.2407\n",
            "Epoch 79/300\n",
            "11312/11312 [==============================] - 23s 2ms/step - loss: 3.1142 - accuracy: 0.2522\n",
            "Epoch 80/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.0820 - accuracy: 0.2568\n",
            "Epoch 81/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.0694 - accuracy: 0.2550\n",
            "Epoch 82/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.0447 - accuracy: 0.2635\n",
            "Epoch 83/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.0340 - accuracy: 0.2628\n",
            "Epoch 84/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.0039 - accuracy: 0.2694\n",
            "Epoch 85/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.9815 - accuracy: 0.2788\n",
            "Epoch 86/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.9653 - accuracy: 0.2752\n",
            "Epoch 87/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.9522 - accuracy: 0.2748\n",
            "Epoch 88/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.9285 - accuracy: 0.2854\n",
            "Epoch 89/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.8994 - accuracy: 0.2867\n",
            "Epoch 90/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.8833 - accuracy: 0.2903\n",
            "Epoch 91/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.8596 - accuracy: 0.2964\n",
            "Epoch 92/300\n",
            "11312/11312 [==============================] - 23s 2ms/step - loss: 2.8391 - accuracy: 0.2976\n",
            "Epoch 93/300\n",
            "11312/11312 [==============================] - 23s 2ms/step - loss: 2.8176 - accuracy: 0.3083\n",
            "Epoch 94/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.8032 - accuracy: 0.3091\n",
            "Epoch 95/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.7868 - accuracy: 0.3111\n",
            "Epoch 96/300\n",
            "11312/11312 [==============================] - 23s 2ms/step - loss: 2.7632 - accuracy: 0.3149\n",
            "Epoch 97/300\n",
            "11312/11312 [==============================] - 23s 2ms/step - loss: 2.7413 - accuracy: 0.3198\n",
            "Epoch 98/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.7277 - accuracy: 0.3251\n",
            "Epoch 99/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.7563 - accuracy: 0.3163\n",
            "Epoch 100/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.6968 - accuracy: 0.3312\n",
            "Epoch 101/300\n",
            "11312/11312 [==============================] - 23s 2ms/step - loss: 2.6799 - accuracy: 0.3324\n",
            "Epoch 102/300\n",
            "11312/11312 [==============================] - 23s 2ms/step - loss: 2.6682 - accuracy: 0.3360\n",
            "Epoch 103/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.6423 - accuracy: 0.3420\n",
            "Epoch 104/300\n",
            "11312/11312 [==============================] - 23s 2ms/step - loss: 2.6261 - accuracy: 0.3427\n",
            "Epoch 105/300\n",
            "11312/11312 [==============================] - 23s 2ms/step - loss: 2.6154 - accuracy: 0.3456\n",
            "Epoch 106/300\n",
            "11312/11312 [==============================] - 23s 2ms/step - loss: 2.5934 - accuracy: 0.3498\n",
            "Epoch 107/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.5734 - accuracy: 0.3556\n",
            "Epoch 108/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.5624 - accuracy: 0.3557\n",
            "Epoch 109/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.5425 - accuracy: 0.3599\n",
            "Epoch 110/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.5310 - accuracy: 0.3639\n",
            "Epoch 111/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.5082 - accuracy: 0.3724\n",
            "Epoch 112/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.4978 - accuracy: 0.3667\n",
            "Epoch 113/300\n",
            "11312/11312 [==============================] - 23s 2ms/step - loss: 2.4856 - accuracy: 0.3731\n",
            "Epoch 114/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.4647 - accuracy: 0.3740\n",
            "Epoch 115/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.4451 - accuracy: 0.3828\n",
            "Epoch 116/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.4328 - accuracy: 0.3876\n",
            "Epoch 117/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.4238 - accuracy: 0.3858\n",
            "Epoch 118/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.4116 - accuracy: 0.3894\n",
            "Epoch 119/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.3977 - accuracy: 0.3906\n",
            "Epoch 120/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.3766 - accuracy: 0.3953\n",
            "Epoch 121/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.3659 - accuracy: 0.3980\n",
            "Epoch 122/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.3446 - accuracy: 0.4008\n",
            "Epoch 123/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.3315 - accuracy: 0.4080\n",
            "Epoch 124/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.0324 - accuracy: 0.2333\n",
            "Epoch 125/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 4.2420 - accuracy: 0.1255\n",
            "Epoch 126/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.6416 - accuracy: 0.1797\n",
            "Epoch 127/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.4250 - accuracy: 0.2063\n",
            "Epoch 128/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.1794 - accuracy: 0.2396\n",
            "Epoch 129/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 3.0206 - accuracy: 0.2732\n",
            "Epoch 130/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.8899 - accuracy: 0.2990\n",
            "Epoch 131/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.7825 - accuracy: 0.3151\n",
            "Epoch 132/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.7043 - accuracy: 0.3342\n",
            "Epoch 133/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.6434 - accuracy: 0.3460\n",
            "Epoch 134/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.5765 - accuracy: 0.3579\n",
            "Epoch 135/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.5260 - accuracy: 0.3675\n",
            "Epoch 136/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.4656 - accuracy: 0.3814\n",
            "Epoch 137/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.4241 - accuracy: 0.3915\n",
            "Epoch 138/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.3711 - accuracy: 0.4018\n",
            "Epoch 139/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.3228 - accuracy: 0.4100\n",
            "Epoch 140/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.3012 - accuracy: 0.4133\n",
            "Epoch 141/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.2637 - accuracy: 0.4204\n",
            "Epoch 142/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.2129 - accuracy: 0.4323\n",
            "Epoch 143/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.1681 - accuracy: 0.4470\n",
            "Epoch 144/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.1265 - accuracy: 0.4501\n",
            "Epoch 145/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.1120 - accuracy: 0.4564\n",
            "Epoch 146/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.1017 - accuracy: 0.4554\n",
            "Epoch 147/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.0702 - accuracy: 0.4613\n",
            "Epoch 148/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.0201 - accuracy: 0.4766\n",
            "Epoch 149/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.9726 - accuracy: 0.4856\n",
            "Epoch 150/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.9241 - accuracy: 0.4992\n",
            "Epoch 151/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.8844 - accuracy: 0.5099\n",
            "Epoch 152/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.8560 - accuracy: 0.5117\n",
            "Epoch 153/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.8709 - accuracy: 0.5092\n",
            "Epoch 154/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.4112 - accuracy: 0.4007\n",
            "Epoch 155/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.2503 - accuracy: 0.4145\n",
            "Epoch 156/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.1036 - accuracy: 0.4531\n",
            "Epoch 157/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 2.0219 - accuracy: 0.4750\n",
            "Epoch 158/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.9655 - accuracy: 0.4860\n",
            "Epoch 159/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.9052 - accuracy: 0.5033\n",
            "Epoch 160/300\n",
            "11312/11312 [==============================] - 30s 3ms/step - loss: 1.8560 - accuracy: 0.5157\n",
            "Epoch 161/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.8161 - accuracy: 0.5277\n",
            "Epoch 162/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.7773 - accuracy: 0.5351\n",
            "Epoch 163/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.7328 - accuracy: 0.5489\n",
            "Epoch 164/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.6936 - accuracy: 0.5547\n",
            "Epoch 165/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.6614 - accuracy: 0.5617\n",
            "Epoch 166/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.6180 - accuracy: 0.5762\n",
            "Epoch 167/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.5673 - accuracy: 0.5890\n",
            "Epoch 168/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.5378 - accuracy: 0.6009\n",
            "Epoch 169/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.4986 - accuracy: 0.6094\n",
            "Epoch 170/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.4605 - accuracy: 0.6213\n",
            "Epoch 171/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.4409 - accuracy: 0.6229\n",
            "Epoch 172/300\n",
            "11312/11312 [==============================] - 25s 2ms/step - loss: 1.3909 - accuracy: 0.6339\n",
            "Epoch 173/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.3689 - accuracy: 0.6431\n",
            "Epoch 174/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.3235 - accuracy: 0.6551\n",
            "Epoch 175/300\n",
            "11312/11312 [==============================] - 25s 2ms/step - loss: 1.2997 - accuracy: 0.6593\n",
            "Epoch 176/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.2547 - accuracy: 0.6724\n",
            "Epoch 177/300\n",
            "11312/11312 [==============================] - 25s 2ms/step - loss: 1.2102 - accuracy: 0.6888\n",
            "Epoch 178/300\n",
            "11312/11312 [==============================] - 25s 2ms/step - loss: 1.1738 - accuracy: 0.6953\n",
            "Epoch 179/300\n",
            "11312/11312 [==============================] - 25s 2ms/step - loss: 1.1439 - accuracy: 0.7059\n",
            "Epoch 180/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.1263 - accuracy: 0.7118\n",
            "Epoch 181/300\n",
            "11312/11312 [==============================] - 25s 2ms/step - loss: 1.0856 - accuracy: 0.7194\n",
            "Epoch 182/300\n",
            "11312/11312 [==============================] - 25s 2ms/step - loss: 1.0447 - accuracy: 0.7358\n",
            "Epoch 183/300\n",
            "11312/11312 [==============================] - 25s 2ms/step - loss: 1.0324 - accuracy: 0.7327\n",
            "Epoch 184/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.9945 - accuracy: 0.7481\n",
            "Epoch 185/300\n",
            "11312/11312 [==============================] - 31s 3ms/step - loss: 0.9675 - accuracy: 0.7562\n",
            "Epoch 186/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.9298 - accuracy: 0.7646\n",
            "Epoch 187/300\n",
            "11312/11312 [==============================] - 25s 2ms/step - loss: 0.8868 - accuracy: 0.7802\n",
            "Epoch 188/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.8636 - accuracy: 0.7862\n",
            "Epoch 189/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.8445 - accuracy: 0.7905\n",
            "Epoch 190/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.8058 - accuracy: 0.8016\n",
            "Epoch 191/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.8002 - accuracy: 0.8040\n",
            "Epoch 192/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.7496 - accuracy: 0.8198\n",
            "Epoch 193/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.7266 - accuracy: 0.8270\n",
            "Epoch 194/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.7132 - accuracy: 0.8293\n",
            "Epoch 195/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.6818 - accuracy: 0.8382\n",
            "Epoch 196/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.6387 - accuracy: 0.8553\n",
            "Epoch 197/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.6038 - accuracy: 0.8643\n",
            "Epoch 198/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.5819 - accuracy: 0.8702\n",
            "Epoch 199/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.5538 - accuracy: 0.8800\n",
            "Epoch 200/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.5182 - accuracy: 0.8892\n",
            "Epoch 201/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.4850 - accuracy: 0.8988\n",
            "Epoch 202/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.4596 - accuracy: 0.9063\n",
            "Epoch 203/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.4436 - accuracy: 0.9122\n",
            "Epoch 204/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.4349 - accuracy: 0.9114\n",
            "Epoch 205/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.4240 - accuracy: 0.9163\n",
            "Epoch 206/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.3986 - accuracy: 0.9276\n",
            "Epoch 207/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.3987 - accuracy: 0.9235\n",
            "Epoch 208/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.3616 - accuracy: 0.9345\n",
            "Epoch 209/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.3498 - accuracy: 0.9367\n",
            "Epoch 210/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.3395 - accuracy: 0.9412\n",
            "Epoch 211/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.3342 - accuracy: 0.9426\n",
            "Epoch 212/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.3530 - accuracy: 0.9344\n",
            "Epoch 213/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.3288 - accuracy: 0.9397\n",
            "Epoch 214/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.2808 - accuracy: 0.9537\n",
            "Epoch 215/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.2268 - accuracy: 0.9697\n",
            "Epoch 216/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.2014 - accuracy: 0.9783\n",
            "Epoch 217/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.1738 - accuracy: 0.9836\n",
            "Epoch 218/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.1549 - accuracy: 0.9874\n",
            "Epoch 219/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.1413 - accuracy: 0.9885\n",
            "Epoch 220/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.1257 - accuracy: 0.9926\n",
            "Epoch 221/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.1154 - accuracy: 0.9929\n",
            "Epoch 222/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.1095 - accuracy: 0.9942\n",
            "Epoch 223/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.1025 - accuracy: 0.9947\n",
            "Epoch 224/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0940 - accuracy: 0.9962\n",
            "Epoch 225/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0861 - accuracy: 0.9971\n",
            "Epoch 226/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0960 - accuracy: 0.9953\n",
            "Epoch 227/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.1466 - accuracy: 0.9823\n",
            "Epoch 228/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.4441 - accuracy: 0.8855\n",
            "Epoch 229/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.7386 - accuracy: 0.7855\n",
            "Epoch 230/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.4929 - accuracy: 0.8622\n",
            "Epoch 231/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.2245 - accuracy: 0.9544\n",
            "Epoch 232/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.1043 - accuracy: 0.9925\n",
            "Epoch 233/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0646 - accuracy: 0.9987\n",
            "Epoch 234/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0523 - accuracy: 0.9994\n",
            "Epoch 235/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0451 - accuracy: 0.9996\n",
            "Epoch 236/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0405 - accuracy: 0.9998\n",
            "Epoch 237/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0375 - accuracy: 0.9998\n",
            "Epoch 238/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0343 - accuracy: 1.0000\n",
            "Epoch 239/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0317 - accuracy: 1.0000\n",
            "Epoch 240/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0297 - accuracy: 1.0000\n",
            "Epoch 241/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0283 - accuracy: 1.0000\n",
            "Epoch 242/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0261 - accuracy: 1.0000\n",
            "Epoch 243/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0247 - accuracy: 1.0000\n",
            "Epoch 244/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0234 - accuracy: 1.0000\n",
            "Epoch 245/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0219 - accuracy: 1.0000\n",
            "Epoch 246/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0209 - accuracy: 1.0000\n",
            "Epoch 247/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0196 - accuracy: 1.0000\n",
            "Epoch 248/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0187 - accuracy: 1.0000\n",
            "Epoch 249/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0176 - accuracy: 1.0000\n",
            "Epoch 250/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0168 - accuracy: 1.0000\n",
            "Epoch 251/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0158 - accuracy: 1.0000\n",
            "Epoch 252/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0151 - accuracy: 1.0000\n",
            "Epoch 253/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0143 - accuracy: 1.0000\n",
            "Epoch 254/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0136 - accuracy: 1.0000\n",
            "Epoch 255/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0129 - accuracy: 1.0000\n",
            "Epoch 256/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0122 - accuracy: 1.0000\n",
            "Epoch 257/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0118 - accuracy: 1.0000\n",
            "Epoch 258/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0117 - accuracy: 0.9999\n",
            "Epoch 259/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0168 - accuracy: 0.9995\n",
            "Epoch 260/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.5150 - accuracy: 0.8683\n",
            "Epoch 261/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 1.3280 - accuracy: 0.6462\n",
            "Epoch 262/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.4393 - accuracy: 0.8723\n",
            "Epoch 263/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.1430 - accuracy: 0.9708\n",
            "Epoch 264/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0544 - accuracy: 0.9953\n",
            "Epoch 265/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0274 - accuracy: 0.9993\n",
            "Epoch 266/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0193 - accuracy: 1.0000\n",
            "Epoch 267/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0167 - accuracy: 1.0000\n",
            "Epoch 268/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0150 - accuracy: 1.0000\n",
            "Epoch 269/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0138 - accuracy: 1.0000\n",
            "Epoch 270/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0128 - accuracy: 1.0000\n",
            "Epoch 271/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0120 - accuracy: 1.0000\n",
            "Epoch 272/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0113 - accuracy: 1.0000\n",
            "Epoch 273/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0107 - accuracy: 1.0000\n",
            "Epoch 274/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0101 - accuracy: 1.0000\n",
            "Epoch 275/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0096 - accuracy: 1.0000\n",
            "Epoch 276/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0091 - accuracy: 1.0000\n",
            "Epoch 277/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0087 - accuracy: 1.0000\n",
            "Epoch 278/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0083 - accuracy: 1.0000\n",
            "Epoch 279/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 280/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 281/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 282/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0070 - accuracy: 1.0000\n",
            "Epoch 283/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 284/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 285/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 286/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 287/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 288/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 289/300\n",
            "11312/11312 [==============================] - 25s 2ms/step - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 290/300\n",
            "11312/11312 [==============================] - 24s 2ms/step - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 291/300\n",
            "11312/11312 [==============================] - 25s 2ms/step - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 292/300\n",
            "11312/11312 [==============================] - 25s 2ms/step - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 293/300\n",
            "11312/11312 [==============================] - 26s 2ms/step - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 294/300\n",
            "11312/11312 [==============================] - 25s 2ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 295/300\n",
            "11312/11312 [==============================] - 25s 2ms/step - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 296/300\n",
            "11312/11312 [==============================] - 25s 2ms/step - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 297/300\n",
            "11312/11312 [==============================] - 25s 2ms/step - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 298/300\n",
            "11312/11312 [==============================] - 26s 2ms/step - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 299/300\n",
            "11312/11312 [==============================] - 26s 2ms/step - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 300/300\n",
            "11312/11312 [==============================] - 26s 2ms/step - loss: 0.0035 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f78cf1e9e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJcFe_t7UtUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import randint\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tfJOIVQU7Ep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
        "    '''\n",
        "    INPUTS:\n",
        "    model : model that was trained on text data\n",
        "    tokenizer : tokenizer that was fit on text data\n",
        "    seq_len : length of training sequence\n",
        "    seed_text : raw string text to serve as the seed\n",
        "    num_gen_words : number of words to be generated by model\n",
        "    '''\n",
        "    \n",
        "    # Final Output\n",
        "    output_text = []\n",
        "    \n",
        "    # Intial Seed Sequence\n",
        "    input_text = seed_text\n",
        "    \n",
        "    # Create num_gen_words\n",
        "    for i in range(num_gen_words):\n",
        "        \n",
        "        # Take the input text string and encode it to a sequence\n",
        "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
        "        \n",
        "        # Pad sequences to our trained rate (50 words in the video)\n",
        "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
        "        \n",
        "        # Predict Class Probabilities for each word\n",
        "        pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]\n",
        "        \n",
        "        # Grab word\n",
        "        pred_word = tokenizer.index_word[pred_word_ind] \n",
        "        \n",
        "        # Update the sequence of input text (shifting one over with the new word)\n",
        "        input_text += ' ' + pred_word\n",
        "        \n",
        "        output_text.append(pred_word)\n",
        "        \n",
        "    # Make it look like a sentence.\n",
        "    return ' '.join(output_text)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mHqAeZuvj2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "random.seed(101)\n",
        "random_pick = random.randint(0,len(text_sequences))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtH3o6P-vkTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_seed_text = text_sequences[random_pick]"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3FCXpHKwdpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_text = ' '.join(random_seed_text)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obZxo5iavkZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5d0a0023-f7df-472c-b1dd-2a5d33c845c2"
      },
      "source": [
        "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=50)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"to be afraid of him better sleep with a sober cannibal than a drunken christian landlord said i tell him to stash his tomahawk there or pipe or whatever you call it tell him to stop smoking in short and i will turn in with him but i do n't\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GTjAz3Vwsg6",
        "colab_type": "text"
      },
      "source": [
        "Exploring Generated Sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MckgpDXAwpgZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "d3d5624d-31d7-40fe-871f-42df0bdef941"
      },
      "source": [
        "for i,word in enumerate(r.split()):\n",
        "    if word == 'stash':\n",
        "        print(' '.join(r.split()[i-20:i+30]))\n",
        "        print('\\n')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "be afraid of him. Better sleep with a sober cannibal than a drunken Christian. \"Landlord,\" said I, \"tell him to stash his tomahawk there, or pipe, or whatever you call it; tell him to stop smoking, in short, and I will turn in with him. But I don't fancy having\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t9bMyE5vkJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaqqgz88vjab",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}